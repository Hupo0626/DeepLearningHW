{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfHN7LajAiTT"
   },
   "source": [
    "# CS-GY 9223-D: Deep Learning Homework 1\n",
    "Due on Friday, 15th February 2019, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually.\n",
    "\n",
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Hupo Tang, ht1073\n",
    "\n",
    "Member 2: Yuan Tang Lin, ytl329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4z9DncmAiTZ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGP7FMFJAiTk"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j65EBOosAiTn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8C8EBctIAiTu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pAFGUl7AiT0"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRKt3EBYAiT2"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        # here layer_dimensions is [3072, 256, 128, 10], 2 hidden layers\n",
    "        # I set num_layers to 2, except input layer and output layer which are No.0,3 layer\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.batch_size = 100\n",
    "        self.devide_ratio = 0.9\n",
    "        self.alpha = 0.01\n",
    "        self.eps = 0.0000001\n",
    "        \n",
    "        self.num_layers = len(layer_dimensions) - 2\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        self.parameters = {} # save layer No.1,2,3 's parameters\n",
    "        for i in range(1, len(layer_dimensions)):\n",
    "            eps = np.sqrt(2.0 / (layer_dimensions[i] + layer_dimensions[i-1]))\n",
    "            W = np.random.randn(layer_dimensions[i], layer_dimensions[i-1]) * eps\n",
    "            b = np.zeros((layer_dimensions[i],))\n",
    "            self.parameters.update({i: {'W': W, 'b': b}})\n",
    "        \n",
    "        # init parameters\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass. W_i, A_(i-1), b_i\n",
    "        \"\"\"\n",
    "        Z = W.dot(A) + np.tile(b, (A.shape[1], 1)).T\n",
    "        cache = {'Z': Z, 'A': A, 'W': W, 'b': b}\n",
    "        if self.drop_prob > 0:\n",
    "            Z, m = self.dropout(Z, self.drop_prob)\n",
    "            cache['m'] = m\n",
    "        return Z, cache\n",
    "        \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return self.relu(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M >= prob) * 1\n",
    "        A = np.multiply(A, M)\n",
    "        return A, M\n",
    "    \n",
    "    def softmax(self, X):\n",
    "#         X = X - X.max()\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        AL = X\n",
    "        cache = {}\n",
    "        for l in range(1, self.num_layers+1):\n",
    "            W = self.parameters[l]['W']\n",
    "            b = self.parameters[l]['b']\n",
    "            Z, _cache = self.affineForward(AL, W, b)\n",
    "            AL = self.activationForward(Z, activation=\"relu\")\n",
    "            cache.update({l:_cache})\n",
    "        last_W, last_b = self.parameters[len(layer_dimensions)-1]['W'], self.parameters[len(layer_dimensions)-1]['b'] # W_3, b_3\n",
    "        last_Z, _cache = self.affineForward(AL, last_W, last_b) # Z_3\n",
    "        cache.update({self.num_layers+1: {'W': last_W, 'b': last_b, 'Z': last_Z, 'A': AL}})\n",
    "        AL = last_Z\n",
    "        return AL, cache\n",
    "    \n",
    "    def one_hot(self, y, num_classes=10):\n",
    "        y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "        y_one_hot[np.arange(y.shape[0]), y] = 1\n",
    "        return np.array(y_one_hot)\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        _y = self.softmax(AL) # size (num_classes, batch_size) here 10*50\n",
    "        y = self.one_hot(y).T # size (num_classes, batch_size) here 10*50\n",
    "#         _y_prob = AL[y, y.shape[0]] # size here 50\n",
    "#         _y = np.argmax(AL, axis=0) # get 50 labels\n",
    "        cost = - (y * (np.log(_y + self.eps)) + (1-y)*np.log(1 - _y + self.eps))\n",
    "        \n",
    "        reg = 0\n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "            penalty = 0\n",
    "            for i in range(1, self.num_layers+1):\n",
    "                penalty += (self.parameters[i]['W'] ** 2).sum()\n",
    "            reg = self.reg_lambda * penalty / 2\n",
    "        \n",
    "        cost = cost.mean() + reg\n",
    "        \n",
    "        # gradient of cost\n",
    "        dAL = (_y - y) / y.shape[0] # dZ_3\n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer. dA_2, dA_1\n",
    "        :param cache: cache returned in affineForward. cache[2], cache[1]\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        dZ = self.activationBackward(dA_prev, cache, activation='relu') # dZ_2, dZ_1\n",
    "        db = np.sum(dZ, axis=1) # db_2, db_1\n",
    "        dW = dZ.dot(cache['A'].T) + self.reg_lambda * cache['W'] # dW_2, dW_1\n",
    "        dA = cache['W'].T.dot(dZ) # dA_1, dx\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        return np.multiply(dA, self.relu_derivative(dA, cache['Z']))\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        dx = 1.0 * (cached_x>0)\n",
    "        return dx\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "        dA = np.multiply(dA, cache['m'])\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        dZ = dAL # dZ_3, activation derivative is calculated in def costFunction\n",
    "        dW = dZ.dot(cache[3]['A'].T) + self.reg_lambda * cache[3]['W'] # dW_3\n",
    "        db = np.sum(dZ, axis=1) # db_3\n",
    "        dA = cache[3]['W'].T.dot(dZ) # dA_2\n",
    "        gradients[3] = {'dW': dW, 'db': db}\n",
    "        for i in range(2, 0, -1):\n",
    "\n",
    "            if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "                dA = self.dropout_backward(dA, cache[i])\n",
    "                \n",
    "            dA, dW, db = self.affineBackward(dA, cache[i])\n",
    "            gradients[i] = {'dW': dW, 'db': db}\n",
    "#         if self.reg_lambda > 0:  # already add during above codes\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        for i in range(1, self.num_layers+1):\n",
    "            self.parameters[i]['W'] -= alpha * gradients[i]['dW']\n",
    "            self.parameters[i]['b'] -= alpha * gradients[i]['db']\n",
    "            \n",
    "    def makeTrainAndValidation(self, X, y, devide_ratio):\n",
    "        p = int(X.shape[1] * devide_ratio)\n",
    "        x_t, y_t = X[:, :p], y[:p]\n",
    "        x_v, y_v = X[:, p:], y[p:]\n",
    "        splited_set = {\n",
    "            'train': (np.array(x_t), np.array(y_t)),\n",
    "            'validation': (np.array(x_v), np.array(y_v))\n",
    "        }\n",
    "        return splited_set\n",
    "    \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        data_set = self.makeTrainAndValidation(X, y, self.devide_ratio)\n",
    "        for e in range(0, iters):\n",
    "            # get minibatch\n",
    "            for phase in ['train', 'validation']:\n",
    "                phase_X, phase_y = data_set[phase]\n",
    "                X_batch, y_batch = self.get_batch(phase_X, phase_y, batch_size)\n",
    "                \n",
    "                overall_loss, overall_corrects = 0.0, 0\n",
    "                num_batches = phase_X.shape[1] // batch_size\n",
    "#                 flag = num_batches // 100\n",
    "                \n",
    "                for i, data in enumerate(zip(X_batch, y_batch)):\n",
    "                    x_minibatch, y_minibatch = data\n",
    "                \n",
    "            # forward prop\n",
    "                    AL, cache = self.forwardPropagation(x_minibatch)\n",
    "                \n",
    "            # compute loss\n",
    "                    batch_loss, dAL = self.costFunction(AL, y_minibatch)\n",
    "                    _y = self.softmax(AL)\n",
    "                    batch_corrects = np.sum(np.argmax(_y, axis=0)==y_minibatch)\n",
    "                    \n",
    "                    overall_loss += batch_loss\n",
    "                    overall_corrects += batch_corrects\n",
    "                    \n",
    "            # compute gradients\n",
    "                    if phase=='train':\n",
    "                        g = self.backPropagation(dAL, y_batch, cache)\n",
    "                    \n",
    "            # update weights and biases based on gradient\n",
    "                        self.updateParameters(g, alpha)\n",
    "                        \n",
    "            if e % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                print(\"Epoch {:3d} , phase {:10} finished, global_loss:{:6.4f}, global_acc:{:6.4f}%\".format(e+1, phase, overall_loss/i, 100*overall_corrects/(i*batch_size)))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = self.forwardPropagation(X)\n",
    "        y_pred = self.softmax(AL)\n",
    "        y_pred = self.ont_hot(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        # resort X,y\n",
    "        n = X.shape[1]\n",
    "        perm = np.random.permutation(n)\n",
    "        X = X[:, perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        X_b, y_b = [], []\n",
    "        for i in range(0, n, batch_size):\n",
    "            _x = X[:, i:i+batch_size]\n",
    "            _y = y[i:i+batch_size]\n",
    "            X_b.append(_x)\n",
    "            y_b.append(_y)\n",
    "        X_batch, y_batch = np.array(X_b), np.array(y_b)\n",
    "        return X_batch, y_batch\n",
    "\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaintang/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:264: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/joaintang/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:264: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 101 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 201 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 301 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 401 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 501 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 601 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 701 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 801 , phase validation finished, global_loss:   inf, global_acc:   inf%\n",
      "Epoch 901 , phase validation finished, global_loss:   inf, global_acc:   inf%\n"
     ]
    }
   ],
   "source": [
    "traindataiter = iter(trainloader)\n",
    "X_train, y_train = traindataiter.next()\n",
    "X_train, y_train = X_train.numpy(), y_train.numpy()\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]*X_train.shape[3]).T\n",
    "\n",
    "layer_dimensions = [X_train.shape[0], 256, 128, 10]\n",
    "MyNN = NeuralNetwork(layer_dimensions, drop_prob=0.03, reg_lambda=0.000001)\n",
    "MyNN.train(X_train, y_train)\n",
    "\n",
    "# for _ in range(len(trainloader)):\n",
    "#     MyNN.train(X_train, y_train)\n",
    "#     X_train, y_train = traindataiter.next()\n",
    "#     X_train, y_train = X_train.numpy(), y_train.numpy()\n",
    "#     X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]*X_train.shape[3]).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW1-fullyConnected-uni.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
