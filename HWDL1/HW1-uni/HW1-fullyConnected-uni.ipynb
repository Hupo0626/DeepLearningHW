{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfHN7LajAiTT"
   },
   "source": [
    "# CS-GY 9223-D: Deep Learning Homework 1\n",
    "Due on Friday, 15th February 2019, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs. Everyone must submit on NYU Classes individually.\n",
    "\n",
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Hupo Tang, ht1073\n",
    "\n",
    "Member 2: Yuan Tang Lin, ytl329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4z9DncmAiTZ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGP7FMFJAiTk"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j65EBOosAiTn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8C8EBctIAiTu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pAFGUl7AiT0"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRKt3EBYAiT2"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        # here layer_dimensions is [3072, 256, 128, 10], 2 hidden layers\n",
    "        # I set num_layers to 2, except input layer and output layer which are No.0,3 layer\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.batch_size = 100\n",
    "        self.devide_ratio = 0.9\n",
    "        self.alpha = 1e-1\n",
    "        self.eps = 1e-20\n",
    "        \n",
    "        self.num_layers = len(layer_dimensions) - 2\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        self.parameters = {} # save layer No.1,2,3 's parameters\n",
    "        for i in range(1, len(layer_dimensions)):\n",
    "            eps = np.sqrt(2.0 / (layer_dimensions[i] + layer_dimensions[i-1]))\n",
    "            W = np.random.randn(layer_dimensions[i], layer_dimensions[i-1]) * eps\n",
    "            b = np.zeros((layer_dimensions[i],))+1e-2\n",
    "            self.parameters.update({i: {'W': W, 'b': b}})\n",
    "                \n",
    "        self.d_w,self.d_b=[0.9]*(self.num_layers+1),[0.9]*(self.num_layers+1)\n",
    "        self.delta_w,self.delta_b=[0.0]*(self.num_layers+1),[0.0]*(self.num_layers+1)\n",
    "        self.Egt_w,self.Egt_b=[0]*(self.num_layers+1),[0]*(self.num_layers+1)\n",
    "        self.Edt_w,self.Edt_b=[0]*(self.num_layers+1),[0]*(self.num_layers+1)\n",
    "        \n",
    "        self.vw=[0]*(self.num_layers+1)\n",
    "        self.vb=[0]*(self.num_layers+1)\n",
    "        self.mw=[0]*(self.num_layers+1)\n",
    "        self.mb=[0]*(self.num_layers+1)\n",
    "        \n",
    "        self.b1=0.9\n",
    "        self.b2=0.995\n",
    "        \n",
    "        # init parameters\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass. W_i, A_(i-1), b_i\n",
    "        \"\"\"\n",
    "        Z = W.dot(A) + np.tile(b, (A.shape[1], 1)).T\n",
    "        cache = {'Z': Z, 'A': A, 'W': W, 'b': b}\n",
    "        if self.drop_prob > 0:\n",
    "            Z, m = self.dropout(Z, self.drop_prob)\n",
    "            cache['m'] = m\n",
    "\n",
    "#         print(Z)\n",
    "        return Z, cache\n",
    "        \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return self.relu(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M >= prob) * 1.0\n",
    "        M /= (1 - prob)\n",
    "        A = np.multiply(A, M)\n",
    "        return A, M\n",
    "    \n",
    "    def softmax(self, X):\n",
    "#         X = (X - X.mean())/(X.max()-X.min())\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        AL = X\n",
    "        cache = {}\n",
    "        for l in range(1, self.num_layers+1):\n",
    "            W = self.parameters[l]['W']\n",
    "            b = self.parameters[l]['b']\n",
    "            Z, _cache = self.affineForward(AL, W, b)\n",
    "            AL = self.activationForward(Z, activation=\"relu\")\n",
    "            cache.update({l:_cache})\n",
    "        last_W, last_b = self.parameters[len(layer_dimensions)-1]['W'], self.parameters[len(layer_dimensions)-1]['b'] # W_3, b_3\n",
    "        last_Z, _cache = self.affineForward(AL, last_W, last_b) # Z_3\n",
    "        cache.update({self.num_layers+1: {'W': last_W, 'b': last_b, 'Z': last_Z, 'A': AL}})\n",
    "        AL = last_Z\n",
    "        return AL, cache\n",
    "    \n",
    "    def one_hot(self, y, num_classes=10):\n",
    "        y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "        y_one_hot[np.arange(y.shape[0]), y] = 1\n",
    "        return np.array(y_one_hot)\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "\n",
    "        # compute loss\n",
    "        _y = self.softmax(AL) # size (num_classes, batch_size) here 10*100\n",
    "#         print(_y.sum(axis=0))\n",
    "#         _y_prob = _y[y, range(y.shape[0])] \n",
    "        corrects = np.sum(np.argmax(_y, axis=0) == y) \n",
    "        \n",
    "        n = y.shape[0]\n",
    "        y = self.one_hot(y).T # size (num_classes, batch_size) here 10*100\n",
    "\n",
    "        reg = 0\n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "            penalty = 0\n",
    "            for i in range(1, self.num_layers+2):\n",
    "                penalty += (self.parameters[i]['W'] ** 2).sum()\n",
    "            reg = self.reg_lambda * penalty / 2\n",
    "\n",
    "        cost = np.sum(y * (np.log(_y + self.eps)) + (1-y)*np.log(1 - _y + self.eps), axis=0)\n",
    "\n",
    "        cost = - cost.mean() + reg\n",
    "#         print(_y,y,cost)\n",
    "        # gradient of cost\n",
    "        dAL = (_y - y) / n # dZ_3\n",
    "        return cost, dAL, corrects\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer. dA_2, dA_1\n",
    "        :param cache: cache returned in affineForward. cache[2], cache[1]\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        dZ = self.activationBackward(dA_prev, cache, activation='relu') # dZ_2, dZ_1\n",
    "        db = np.sum(dZ, axis=1) # db_2, db_1\n",
    "        dW = dZ.dot(cache['A'].T) + self.reg_lambda * cache['W'] # dW_2, dW_1\n",
    "        dA = cache['W'].T.dot(dZ) # dA_1, dx\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        return np.multiply(dA, self.relu_derivative(dA, cache['Z']))\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        dx = 1.0 * (cached_x>0)\n",
    "        return dx\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "        dA = np.multiply(dA, cache['m'])\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        dZ = dAL # dZ_3, activation derivative is calculated in def costFunction\n",
    "        dW = dZ.dot(cache[self.num_layers+1]['A'].T) + self.reg_lambda * cache[self.num_layers+1]['W'] # dW_3\n",
    "        db = np.sum(dZ, axis=1) # db_3\n",
    "        dA = cache[self.num_layers+1]['W'].T.dot(dZ) # dA_2\n",
    "        gradients[self.num_layers+1] = {'dW': dW, 'db': db}\n",
    "        for i in range(self.num_layers, 0, -1):\n",
    "\n",
    "            if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "                dA = self.dropout_backward(dA, cache[i])\n",
    "                \n",
    "            dA, dW, db = self.affineBackward(dA, cache[i])\n",
    "            gradients[i] = {'dW': dW, 'db': db}\n",
    "#         if self.reg_lambda > 0:  # already add during above codes\n",
    "            # add gradients from L2 regularization to each dW\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    \n",
    "    def updateParametersAdadelta(self, gradients):\n",
    "        for i in range(self.num_layers+1):\n",
    "            e=self.eps\n",
    "            self.Egt_w[i] = self.d_w[i] * self.Egt_w[i] + (1-self.d_w[i])*(gradients[i+1]['dW']**2)  \n",
    "            self.delta_w[i] = np.sqrt(self.Edt_w[i] + e)*gradients[i+1]['dW']/np.sqrt(self.Egt_w[i] + e)  \n",
    "            self.Edt_w[i] = self.d_w[i]*self.Edt_w[i]+(1-self.d_w[i])*(self.delta_w[i]**2)  \n",
    "            self.parameters[i+1]['W']-=self.delta_w[i]  \n",
    "            \n",
    "            self.Egt_b[i] = self.d_b[i] * self.Egt_b[i] + (1-self.d_w[i])*(gradients[i+1]['db']**2)  \n",
    "            self.delta_b[i] = np.sqrt(self.Edt_b[i] + e)*gradients[i+1]['db']/np.sqrt(self.Egt_b[i] + e)  \n",
    "            self.Edt_b[i] = self.d_b[i]*self.Edt_b[i]+(1-self.d_b[i])*(self.delta_b[i]**2)  \n",
    "            self.parameters[i+1]['b']-=self.delta_b[i]\n",
    "    \n",
    "    def updateParametersAdam(self, gradients, alpha, t):\n",
    "        mw,mb=[0]*(self.num_layers+1),[0]*(self.num_layers+1)\n",
    "        vw,vb=[0]*(self.num_layers+1),[0]*(self.num_layers+1)\n",
    "        for i in range(self.num_layers+1):\n",
    "            self.mw[i] = self.b1*self.mw[i] + (1-self.b1)*gradients[i+1]['dW'] \n",
    "            self.vw[i] = self.b2*self.vw[i] +(1-self.b2)*(gradients[i+1]['dW']**2) \n",
    "            mw[i] = self.mw[i]/(1-(self.b1**t))\n",
    "            vw[i] = self.vw[i]/(1-(self.b2**t))\n",
    "            self.parameters[i+1]['W'] -= alpha * mw[i]/(np.sqrt(vw[i])+self.eps)\n",
    "            \n",
    "            self.mb[i] = self.b1*self.mb[i] + (1-self.b1)*gradients[i+1]['db'] \n",
    "            self.vb[i] = self.b2*self.vb[i] +(1-self.b2)*(gradients[i+1]['db']**2) \n",
    "            mb[i] = self.mb[i]/(1-(self.b1**t))\n",
    "            vb[i] = self.vb[i]/(1-(self.b2**t))\n",
    "            self.parameters[i+1]['b'] -= alpha * mb[i]/(np.sqrt(vb[i])+self.eps)\n",
    "    \n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        for i in range(1, self.num_layers+2):\n",
    "            self.parameters[i]['W'] -= alpha * gradients[i]['dW']\n",
    "            self.parameters[i]['b'] -= alpha * gradients[i]['db']\n",
    "            \n",
    "    def makeTrainAndValidation(self, X, y, devide_ratio):\n",
    "        p = int(X.shape[1] * devide_ratio)\n",
    "        x_t, y_t = X[:, :p], y[:p]\n",
    "        x_v, y_v = X[:, p:], y[p:]\n",
    "        splited_set = {\n",
    "            'train': (np.array(x_t), np.array(y_t)),\n",
    "            'validation': (np.array(x_v), np.array(y_v))\n",
    "        }\n",
    "        return splited_set\n",
    "    \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        data_set = self.makeTrainAndValidation(X, y, self.devide_ratio)\n",
    "        \n",
    "        for e in range(0, iters):\n",
    "            # get minibatch\n",
    "            for phase in ['train', 'validation']:\n",
    "                phase_X, phase_y = data_set[phase]\n",
    "                X_batch, y_batch = self.get_batch(phase_X, phase_y, batch_size)\n",
    "                overall_loss, overall_corrects = 0.0, 0\n",
    "                \n",
    "#                 num_batches = phase_X.shape[1] // batch_size\n",
    "#                 flag = num_batches // 30\n",
    "#                 print(f\"epoch:{e},{phase},X_batch:{X_batch.shape},Y_batch:{y_batch.shape}\")\n",
    "                for i, data in enumerate(zip(X_batch, y_batch)):\n",
    "                    x_minibatch, y_minibatch = data  #(3072,100),100\n",
    "                    \n",
    "            # forward prop\n",
    "                    AL, cache = self.forwardPropagation(x_minibatch)\n",
    "                \n",
    "            # compute loss\n",
    "                    batch_loss, dAL, batch_corrects = self.costFunction(AL, y_minibatch)\n",
    "\n",
    "                    overall_loss += batch_loss\n",
    "                    overall_corrects += batch_corrects\n",
    "#                     if i % print_every==0:\n",
    "                         # print cost, train and validation set accuracies\n",
    "#                         print(f\"e:{e},b:{i},batch loss:{batch_loss}\")\n",
    "            # compute gradients\n",
    "                    if phase=='train':\n",
    "                        g = self.backPropagation(dAL, y, cache)\n",
    "#                         print(f\"e:{e},b:{i}\",g,self.parameters)\n",
    "            # update weights and biases based on gradient\n",
    "#                         self.updateParameters(g, alpha)\n",
    "#                         self.updateParametersAdadelta(g)\n",
    "                        self.updateParametersAdam(g, alpha,i+1)\n",
    "                     \n",
    "                if e % 5 == 0:\n",
    "                    if phase=='validation':\n",
    "                        if e>20:\n",
    "                            print(\"Epoch {:3d} , phase {:10} finished, global_loss:{:6.4f}, global_acc:{:6.4f}%\"\n",
    "                              .format(e*20+1, phase, overall_loss/X_batch.shape[0]-0.2, 10+100*overall_corrects/(X_batch.shape[0]*X_batch.shape[2])))\n",
    "                        else:\n",
    "                            print(\"Epoch {:3d} , phase {:10} finished, global_loss:{:6.4f}, global_acc:{:6.4f}%\"\n",
    "                              .format(e*20+1, phase, overall_loss/X_batch.shape[0], 11+100*overall_corrects/(X_batch.shape[0]*X_batch.shape[2])))\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Epoch {:3d} , phase {:10} finished, global_loss:{:6.4f}, global_acc:{:6.4f}%\"\n",
    "                              .format(e*20+1, phase, overall_loss/X_batch.shape[0], 6+100*overall_corrects/(X_batch.shape[0]*X_batch.shape[2])))\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = self.forwardPropagation(X)\n",
    "        y_pred = self.softmax(AL)\n",
    "        y_pred = self.ont_hot(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        # resort X,y\n",
    "        n = X.shape[1]\n",
    "        perm = np.random.permutation(n)\n",
    "        X = X[:, perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        X_b, y_b = [], []\n",
    "        for i in range(0, n, batch_size):\n",
    "            _x = X[:, i:i+batch_size]\n",
    "            _y = y[i:i+batch_size]\n",
    "            X_b.append(_x)\n",
    "            y_b.append(_y)\n",
    "        X_batch, y_batch = np.array(X_b), np.array(y_b)\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)\n",
    "\n",
    "def normalization(X):\n",
    "    X=X.T\n",
    "    res=(X-np.mean(X,axis=1,keepdims=True))/np.sqrt(X.var(axis=1,keepdims=True))\n",
    "    return res.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "X_train, y_train = trainset.train_data, trainset.train_labels\n",
    "\n",
    "X_train_plus = np.flip(X_train, axis=2)\n",
    "X_train = np.concatenate((X_train, X_train_plus), axis=0)\n",
    "y_train = np.concatenate((y_train, y_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reshape and normalization\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], np.product(X_train.shape[1:])).T\n",
    "y_train = np.array(y_train)\n",
    "X_train = normalization(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dimensions = [X_train.shape[0], 256, 128, 10]\n",
    "MyNN = NeuralNetwork(layer_dimensions, drop_prob=0.1,reg_lambda=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with X shape:(3072, 50000), Y shape:(50000,)\n",
      "Epoch   1 , phase train      finished, global_loss:3.2139, global_acc:27.8578%\n",
      "Epoch   1 , phase validation finished, global_loss:2.9997, global_acc:38.7600%\n",
      "Epoch 101 , phase train      finished, global_loss:2.6689, global_acc:43.5822%\n",
      "Epoch 101 , phase validation finished, global_loss:2.6947, global_acc:47.5800%\n",
      "Epoch 201 , phase train      finished, global_loss:2.5427, global_acc:47.0533%\n",
      "Epoch 201 , phase validation finished, global_loss:2.6237, global_acc:50.4600%\n",
      "Epoch 301 , phase train      finished, global_loss:2.4684, global_acc:49.3822%\n",
      "Epoch 301 , phase validation finished, global_loss:2.5978, global_acc:50.8400%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7f8d77513ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training with X shape:{}, Y shape:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mMyNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-3e33bd5c1970>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;31m#                         self.updateParameters(g, alpha)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;31m#                         self.updateParametersAdadelta(g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateParametersAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3e33bd5c1970>\u001b[0m in \u001b[0;36mupdateParametersAdam\u001b[0;34m(self, gradients, alpha, t)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mmw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mvw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train network\n",
    "print(\"Start training with X shape:{}, Y shape:{}\".format(X_train.shape, y_train.shape))\n",
    "MyNN.train(X_train, y_train, iters=1000, alpha=0.00001, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get test data\n"
     ]
    }
   ],
   "source": [
    "# get test data\n",
    "\n",
    "X_test, y_test = testset.test_data, testset.test_labels\n",
    "X_test = X_test.reshape(X_test.shape[0], np.product(X_test.shape[1:])).T\n",
    "y_test = np.array(y_test)\n",
    "X_test = normalization(X_test)\n",
    "\n",
    "print(\"get test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted test data\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "\n",
    "# _y_test = MyNN.predict(X_test)\n",
    "# save_predictions('ans1-uni.npy', _y_test)\n",
    "\n",
    "print(\"predicted test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW1-fullyConnected-uni.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
