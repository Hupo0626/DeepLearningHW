{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, \\\n",
    "    Activation, ZeroPadding2D\n",
    "from keras.layers import add, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, Lambda, Reshape, ZeroPadding2D\n",
    "# from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same'):\n",
    "    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    return x\n",
    "\n",
    "def identity_Block(inpt, nb_filter, kernel_size, strides=(1, 1), with_conv_shortcut=False):\n",
    "    x = Conv2d_BN(inpt, nb_filter=nb_filter, kernel_size=kernel_size, strides=strides, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size, padding='same')\n",
    "    if with_conv_shortcut:\n",
    "        shortcut = Conv2d_BN(inpt, nb_filter=nb_filter, strides=strides, kernel_size=kernel_size)\n",
    "        x = add([x, shortcut])\n",
    "        return x\n",
    "    else:\n",
    "        x = add([x, inpt])\n",
    "        return x\n",
    "\n",
    "def bottleneck_Block(inpt,nb_filters,strides=(1,1),with_conv_shortcut=False):\n",
    "    k1,k2,k3=nb_filters\n",
    "    x = Conv2d_BN(inpt, nb_filter=k1, kernel_size=1, strides=strides, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=k2, kernel_size=3, padding='same')\n",
    "#     x = Dropout(0.2)(x)\n",
    "    x = Conv2d_BN(x, nb_filter=k3, kernel_size=1, padding='same')\n",
    "    if with_conv_shortcut:\n",
    "        shortcut = Conv2d_BN(inpt, nb_filter=k3, strides=strides, kernel_size=1)\n",
    "        x = add([x, shortcut])\n",
    "        return x\n",
    "    else:\n",
    "        x = add([x, inpt])\n",
    "        return x\n",
    "\n",
    "def resnet_50(width,height,channel,classes):\n",
    "    inpt = Input(shape=(width, height, channel))\n",
    "    x = ZeroPadding2D((2, 2))(inpt)\n",
    "    x = Conv2d_BN(x, nb_filter=64, kernel_size=(6, 6), strides=(1, 1), padding='valid')\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    #conv2_x\n",
    "    x = bottleneck_Block(x, nb_filters=[64,64,256],strides=(1,1),with_conv_shortcut=True)\n",
    "    x = bottleneck_Block(x, nb_filters=[64,64,256])\n",
    "#     x = bottleneck_Block(x, nb_filters=[64,64,256])\n",
    "\n",
    "    #conv3_x\n",
    "    x = bottleneck_Block(x, nb_filters=[128, 128, 512],strides=(2,2),with_conv_shortcut=True)\n",
    "    x = bottleneck_Block(x, nb_filters=[128, 128, 512])\n",
    "#     x = bottleneck_Block(x, nb_filters=[128, 128, 512])\n",
    "#     x = bottleneck_Block(x, nb_filters=[128, 128, 512])\n",
    "\n",
    "    #conv4_x\n",
    "    x = bottleneck_Block(x, nb_filters=[256, 256, 1024],strides=(2,2),with_conv_shortcut=True)\n",
    "    x = bottleneck_Block(x, nb_filters=[256, 256, 1024])\n",
    "#     x = bottleneck_Block(x, nb_filters=[256, 256, 1024])\n",
    "#     x = bottleneck_Block(x, nb_filters=[256, 256, 1024])\n",
    "#     x = bottleneck_Block(x, nb_filters=[256, 256, 1024])\n",
    "#     x = bottleneck_Block(x, nb_filters=[256, 256, 1024])\n",
    "\n",
    "    #conv5_x\n",
    "    x = bottleneck_Block(x, nb_filters=[512, 512, 2048], strides=(2, 2), with_conv_shortcut=True)\n",
    "    x = bottleneck_Block(x, nb_filters=[512, 512, 2048])\n",
    "#     x = bottleneck_Block(x, nb_filters=[512, 512, 2048])\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inpt, outputs=x)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 68, 68, 64)\n",
      "(?, 34, 34, 64)\n",
      "(?, 34, 34, 64)\n",
      "(?, 34, 34, 256)\n",
      "(?, 34, 34, 256)\n",
      "(?, 34, 34, 64)\n",
      "(?, 34, 34, 64)\n",
      "(?, 34, 34, 256)\n",
      "(?, 17, 17, 128)\n",
      "(?, 17, 17, 128)\n",
      "(?, 17, 17, 512)\n",
      "(?, 17, 17, 512)\n",
      "(?, 17, 17, 128)\n",
      "(?, 17, 17, 128)\n",
      "(?, 17, 17, 512)\n",
      "(?, 9, 9, 256)\n",
      "(?, 9, 9, 256)\n",
      "(?, 9, 9, 1024)\n",
      "(?, 9, 9, 1024)\n",
      "(?, 9, 9, 256)\n",
      "(?, 9, 9, 256)\n",
      "(?, 9, 9, 1024)\n",
      "(?, 5, 5, 512)\n",
      "(?, 5, 5, 512)\n",
      "(?, 5, 5, 2048)\n",
      "(?, 5, 5, 2048)\n",
      "(?, 5, 5, 512)\n",
      "(?, 5, 5, 512)\n",
      "(?, 5, 5, 2048)\n"
     ]
    }
   ],
   "source": [
    "model = resnet_50(69,69,3,37)\n",
    "# model.summary()\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from scipy.misc import imresize\n",
    "import csv\n",
    "\n",
    "data_path = './'\n",
    "TRAIN = './images_training_revtest/'\n",
    "TEST = './images_test_rev1/'\n",
    "LABELS = './training_solutions_rev1.csv'\n",
    "\n",
    "class data_loader:    \n",
    "    \"\"\"\n",
    "    Creates a class for handling train/valid/test data paths,\n",
    "    training labels and image IDs.\n",
    "    Useful for switching between sample and full datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):    \n",
    "        self.path = path \n",
    "        self.train_path = TRAIN\n",
    "        #self.val_path = path + \"valid\"\n",
    "        self.test_path = TEST\n",
    "        \n",
    "        def get_paths(directory):\n",
    "            return [f for f in os.listdir(directory)]\n",
    "        \n",
    "        self.training_images_paths = get_paths(self.train_path)\n",
    "        # self.validation_images_paths = get_paths(self.val_path)\n",
    "        self.test_images_paths = get_paths(self.test_path)    \n",
    "        \n",
    "        def get_all_solutions():\n",
    "        # Import solutions file and load into self.solutions\n",
    "            all_solutions = {}\n",
    "            # /'training_solutions_rev1.csv'\n",
    "            with open(LABELS, 'r') as f:\n",
    "                reader = csv.reader(f, delimiter=\",\")\n",
    "                next(reader)\n",
    "                for i, line in enumerate(reader):\n",
    "                    all_solutions[line[0]] = [float(x) for x in line[1:]]\n",
    "            return all_solutions\n",
    "        \n",
    "        self.all_solutions = get_all_solutions()\n",
    "\n",
    "    def get_id(self,fname):\n",
    "        return fname.replace(\".jpg\",\"\").replace(\"data\",\"\")\n",
    "        \n",
    "    def find_label(self,val):\n",
    "        return self.all_solutions[val]\n",
    "\n",
    "def process_images(paths,aug=0):\n",
    "    \"\"\"\n",
    "    Import image at 'paths', decode, centre crop and prepare for batching. \n",
    "    \"\"\"\n",
    "    count = len(paths)\n",
    "    arr = np.zeros(shape=(count,3,69,69))\n",
    "    for c, path in enumerate(paths):\n",
    "        raw_img = cv2.imread(path).T  # 3,424,424\n",
    "        img=raw_img\n",
    "        img = img[:,108:315,108:315] #crop 424x424 -> 207x207\n",
    "        img = imresize(img,size=(69,69,3),interp=\"cubic\").T # downsample to half res\n",
    "        arr[c] = img\n",
    "    return arr\n",
    "\n",
    "\n",
    "def BatchGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.training_images_paths:\n",
    "            X_train = process_images([getter.train_path + '/' + fname for fname in [f]])\n",
    "            X_train = np.reshape(X_train,(1,69,69,3))\n",
    "            id_ = getter.get_id(f)\n",
    "            y_train = np.array(getter.find_label(id_))\n",
    "            y_train = np.reshape(y_train,(1,37))\n",
    "            assert(X_train.shape==(1,69,69,3))\n",
    "            yield (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath='./weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size = 32\n",
    "steps_to_take = int(len(fetcher.training_images_paths)*6/batch_size)\n",
    "#val_steps_to_take = int(len(fetcher.validation_images_paths)/batch_size)\n",
    "                #typically be equal to the number of unique samples if your dataset\n",
    "                #divided by the batch size.\n",
    "\n",
    "#model = load_model('tmp/weights.hdf5')\n",
    "\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 48s - loss: 0.0613\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.0295\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.0303\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.0286\n",
      "Epoch 5/10\n",
      " - 29s - loss: 0.0296\n",
      "Epoch 6/10\n",
      " - 29s - loss: 0.0252\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.0174\n",
      "Epoch 8/10\n",
      " - 33s - loss: 0.0149\n",
      "Epoch 9/10\n",
      " - 32s - loss: 0.0126\n",
      "Epoch 10/10\n",
      " - 33s - loss: 0.0139\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=10,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 40s - loss: 0.0695\n",
      "Epoch 2/10\n",
      " - 25s - loss: 0.0290\n",
      "Epoch 3/10\n",
      " - 26s - loss: 0.0298\n",
      "Epoch 4/10\n",
      " - 25s - loss: 0.0291\n",
      "Epoch 5/10\n",
      " - 26s - loss: 0.0292\n",
      "Epoch 6/10\n",
      " - 26s - loss: 0.0283\n",
      "Epoch 7/10\n",
      " - 27s - loss: 0.0263\n",
      "Epoch 8/10\n",
      " - 27s - loss: 0.0266\n",
      "Epoch 9/10\n",
      " - 27s - loss: 0.0272\n",
      "Epoch 10/10\n",
      " - 27s - loss: 0.0281\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=10,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 40s - loss: 0.0582\n",
      "Epoch 2/10\n",
      " - 27s - loss: 0.0294\n",
      "Epoch 3/10\n",
      " - 27s - loss: 0.0294\n",
      "Epoch 4/10\n",
      " - 26s - loss: 0.0291\n",
      "Epoch 5/10\n",
      " - 26s - loss: 0.0282\n",
      "Epoch 6/10\n",
      " - 25s - loss: 0.0267\n",
      "Epoch 7/10\n",
      " - 28s - loss: 0.0233\n",
      "Epoch 8/10\n",
      " - 27s - loss: 0.0234\n",
      "Epoch 9/10\n",
      " - 27s - loss: 0.0247\n",
      "Epoch 10/10\n",
      " - 27s - loss: 0.0254\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=10,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
