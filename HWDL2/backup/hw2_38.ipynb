{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-GY 9223-D: Deep Learning Homework 2\n",
    "Due on Friday, 12th March 2019, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs.\n",
    "\n",
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Hupo Tang, ht1073\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, Lambda, Reshape, ZeroPadding2D\n",
    "# from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import cv2\n",
    "# import matplotlib.pylab as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "TRAIN = './images_training_rev1/'\n",
    "VALID = './images_validation_rev1'\n",
    "TEST = './images_test_rev1/'\n",
    "LABELS = './training_solutions_rev1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do not run twice!!!\n",
    "'''\n",
    "\n",
    "g = glob.glob(data_path+'images_training_rev1/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000):\n",
    "    os.rename(shuf[i], data_path + 'images_validation_rev1/' + shuf[i].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from scipy.misc import imresize\n",
    "import csv\n",
    "\n",
    "class data_loader:    \n",
    "    \"\"\"\n",
    "    Creates a class for handling train/valid/test data paths,\n",
    "    training labels and image IDs.\n",
    "    Useful for switching between sample and full datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):    \n",
    "        self.path = path \n",
    "        self.train_path = TRAIN\n",
    "        self.val_path = VALID\n",
    "        self.test_path = TEST\n",
    "        \n",
    "        def get_paths(directory):\n",
    "            return [f for f in os.listdir(directory)]\n",
    "        \n",
    "        self.training_images_paths = get_paths(self.train_path)\n",
    "        self.validation_images_paths = get_paths(self.val_path)\n",
    "        self.test_images_paths = get_paths(self.test_path)    \n",
    "        \n",
    "        def get_all_solutions():\n",
    "        # Import solutions file and load into self.solutions\n",
    "            all_solutions = {}\n",
    "            # /'training_solutions_rev1.csv'\n",
    "            with open(LABELS, 'r') as f:\n",
    "                reader = csv.reader(f, delimiter=\",\")\n",
    "                next(reader)\n",
    "                for i, line in enumerate(reader):\n",
    "                    all_solutions[line[0]] = [float(x) for x in line[1:]]\n",
    "            return all_solutions\n",
    "        \n",
    "        self.all_solutions = get_all_solutions()\n",
    "\n",
    "    def get_id(self,fname):\n",
    "        return fname.replace(\".jpg\",\"\").replace(\"data\",\"\")\n",
    "        \n",
    "    def find_label(self,val):\n",
    "        return self.all_solutions[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(paths):\n",
    "    \"\"\"\n",
    "    Import image at 'paths', decode, centre crop and prepare for batching. \n",
    "    This is images processing for VGG Model.\n",
    "    \"\"\"\n",
    "    count = len(paths)\n",
    "    arr = np.zeros(shape=(count,3,106,106))\n",
    "    for c, path in enumerate(paths):\n",
    "        img = cv2.imread(path).T\n",
    "        img = img[:,106:106*3,106:106*3] #crop 424x424 -> 212x212\n",
    "        img = imresize(img,size=(106,106,3),interp=\"cubic\").T # downsample to half res\n",
    "        arr[c] = img\n",
    "    return arr\n",
    "\n",
    "def BatchGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.training_images_paths:\n",
    "            X_train = process_images([getter.train_path + '/' + fname for fname in [f]])\n",
    "            X_train = np.reshape(X_train, (1,106,106,3))\n",
    "            id_ = getter.get_id(f)\n",
    "            y_train = np.array(getter.find_label(id_))\n",
    "            y_train = np.reshape(y_train,(1,37))\n",
    "            yield (X_train, y_train)\n",
    "\n",
    "def validGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.validation_images_paths:\n",
    "            X_val = process_images([getter.val_path + '/' + fname for fname in [f]])\n",
    "            X_val = np.reshape(X_val, (1,106,106,3))\n",
    "            id_ = getter.get_id(f)\n",
    "            y_val = np.array(getter.find_label(id_))\n",
    "            y_val = np.reshape(y_val, (1,37))\n",
    "            yield (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images2(paths,aug=0):\n",
    "    \"\"\"\n",
    "    Import image at 'paths', decode, centre crop and prepare for batching. \n",
    "    \"\"\"\n",
    "    count = len(paths)\n",
    "    arr = np.zeros(shape=(count,3,69,69))\n",
    "    for c, path in enumerate(paths):\n",
    "        raw_img = cv2.imread(path).T\n",
    "        if not aug:\n",
    "            img=raw_img\n",
    "        else:\n",
    "            auger={1:rotate90,2:rotate180,3:flip}.get(aug)\n",
    "            img=auger(raw_img)\n",
    "        img = img[:,108:315,108:315] #crop 424x424 -> 207x207\n",
    "        img = imresize(img,size=(69,69,3),interp=\"cubic\").T # downsample to half res\n",
    "        arr[c] = img\n",
    "    return arr\n",
    "\n",
    "def BatchGenerator2(getter):\n",
    "    while 1:\n",
    "        for f in getter.training_images_paths:\n",
    "            for aug in {0,1,2,3,4,5}:\n",
    "                X_train = process_images([getter.train_path + '/' + fname for fname in [f]], aug)\n",
    "                X_train = np.reshape(X_train,(1,69,69,3))\n",
    "                id_ = getter.get_id(f)\n",
    "                y_train = np.array(getter.find_label(id_))\n",
    "                y_train = np.reshape(y_train,(1,37))\n",
    "                assert(X_train.shape==(1,69,69,3))\n",
    "                yield (X_train, y_train)\n",
    "\n",
    "def validGenerator2(getter):\n",
    "    while 1:\n",
    "        for f in getter.validation_images_paths:\n",
    "            X_train = process_images([getter.train_path + '/' + fname for fname in [f]], aug)\n",
    "            X_train = np.reshape(X_train,(1,69,69,3))\n",
    "            id_ = getter.get_id(f)\n",
    "            y_train = np.array(getter.find_label(id_))\n",
    "            y_train = np.reshape(y_train,(1,37))\n",
    "            assert(X_train.shape==(1,69,69,3))\n",
    "            yield (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded the original data by five times by rotating and flippingã€‚\n",
    "\n",
    "def rotate90(img):\n",
    "    return np.rot90(img,axes=(1,2))\n",
    "\n",
    "def rotate180(img):\n",
    "    tmp = np.rot90(img,axes=(1,2))\n",
    "    return np.rot90(tmp,axes=(1,2))\n",
    "\n",
    "def rotate270(img):\n",
    "    tmp = np.rot90(img,axes=(1,2))\n",
    "    tmp = np.rot90(tmp,axes=(1,2))\n",
    "    return np.rot90(tmp,axes=(1,2))\n",
    "\n",
    "def flip_h(img):\n",
    "    return np.flip(img, axis=1)\n",
    "\n",
    "def flip_v(img):\n",
    "    return np.flip(img, axis=2)\n",
    "\n",
    "def process_withaug(paths,aug=0):\n",
    "    count = len(paths)\n",
    "    arr = np.zeros(shape=(count,3,106,106))\n",
    "    for c, path in enumerate(paths):\n",
    "        raw_img = cv2.imread(path).T\n",
    "        if not aug:\n",
    "            img=raw_img\n",
    "        else:\n",
    "            auger={1:rotate90,2:rotate180,3:rotate270,4:flip_h,5:flip_v}.get(aug)\n",
    "            img=auger(raw_img)\n",
    "        img = img[:,106:106*3,106:106*3]\n",
    "        img = imresize(img,size=(106,106,3),interp=\"cubic\").T\n",
    "        arr[c] = img\n",
    "    return arr\n",
    "\n",
    "def BatchGenerator_withaug(getter):\n",
    "    while 1:\n",
    "        for f in getter.training_images_paths:\n",
    "            for aug in {0,1,2,3,4,5}:\n",
    "                X_train = process_withaug([getter.train_path + '/' + fname for fname in [f]], aug)\n",
    "                X_train = np.reshape(X_train, (1,106,106,3))\n",
    "                id_ = getter.get_id(f)\n",
    "                y_train = np.array(getter.find_label(id_))\n",
    "                y_train = np.reshape(y_train,(1,37))\n",
    "                yield (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(layers, model, filters):\n",
    "    \"\"\"\n",
    "    Create a layered Conv/Pooling block\n",
    "    \"\"\"\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))  # zero padding of size 1\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))  # 3x3 filter size \n",
    "    model.add(MaxPooling2D((1,1), strides=(2,2)))\n",
    "\n",
    "def FCBlock(model):\n",
    "    \"\"\"\n",
    "    Fully connected block with ReLU and dropout\n",
    "    \"\"\"\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "def VGG_16():\n",
    "    \"\"\"\n",
    "    Implement VGG16 architecture\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x : x, input_shape=(106,106,3)))\n",
    "    \n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    \n",
    "    model.add(Dense(37, activation = 'sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile \n",
    "optimizer = RMSprop(lr=1e-6)\n",
    "model = VGG_16()\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My CNN design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyCNN():\n",
    "    model = Sequential()\n",
    "    #NHWC(,69,69,3)\n",
    "    model.add(Lambda(lambda x: x, input_shape=(69,69,3)))\n",
    "    print(model.output_shape)    \n",
    "#     model.add(ZeroPadding2D((2,2)))\n",
    "    model.add(Convolution2D(32, 6, 6, activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "#     model.add(ZeroPadding2D((2,2)))\n",
    "    model.add(Convolution2D(64, 5, 5, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(37, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=12, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath='./tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size = 32\n",
    "steps_to_take = int(len(fetcher.training_images_paths)/batch_size)\n",
    "val_steps_to_take = int(len(fetcher.validation_images_paths)/batch_size)\n",
    "                #typically be equal to the number of unique samples if your dataset\n",
    "                #divided by the batch size.\n",
    "\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 with no augmentation\n",
    "\n",
    "hist = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )\n",
    "# 0.021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 with augmentation\n",
    "\n",
    "hist_aug = model.fit_generator(BatchGenerator_withaug(fetcher),\n",
    "                    samples_per_epoch=steps_to_take*3, \n",
    "                    nb_epoch=25,\n",
    "                    verbose=2,\n",
    "                    callbacks=[hist,checkpointer,early_stopping],\n",
    "                   )\n",
    "# 0.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 116s - loss: 0.0173\n",
      "Epoch 2/50\n",
      " - 116s - loss: 0.0170\n",
      "Epoch 3/50\n",
      " - 116s - loss: 0.0163\n",
      "Epoch 4/50\n",
      " - 116s - loss: 0.0166\n",
      "Epoch 5/50\n",
      " - 116s - loss: 0.0168\n",
      "Epoch 6/50\n",
      " - 116s - loss: 0.0169\n",
      "Epoch 7/50\n",
      " - 116s - loss: 0.0168\n",
      "Epoch 8/50\n",
      " - 117s - loss: 0.0165\n",
      "Epoch 9/50\n",
      " - 116s - loss: 0.0164\n",
      "Epoch 10/50\n",
      " - 116s - loss: 0.0167\n",
      "Epoch 11/50\n",
      " - 116s - loss: 0.0163\n",
      "Epoch 12/50\n",
      " - 116s - loss: 0.0161\n",
      "Epoch 13/50\n",
      " - 116s - loss: 0.0164\n",
      "Epoch 14/50\n",
      " - 116s - loss: 0.0164\n",
      "Epoch 15/50\n",
      " - 116s - loss: 0.0165\n",
      "Epoch 16/50\n",
      " - 116s - loss: 0.0160\n",
      "Epoch 17/50\n",
      " - 116s - loss: 0.0155\n",
      "Epoch 18/50\n",
      " - 116s - loss: 0.0164\n",
      "Epoch 19/50\n",
      " - 116s - loss: 0.0156\n",
      "Epoch 20/50\n",
      " - 116s - loss: 0.0158\n",
      "Epoch 21/50\n",
      " - 116s - loss: 0.0155\n",
      "Epoch 22/50\n",
      " - 116s - loss: 0.0162\n",
      "Epoch 23/50\n",
      " - 116s - loss: 0.0161\n",
      "Epoch 24/50\n",
      " - 116s - loss: 0.0156\n",
      "Epoch 25/50\n",
      " - 116s - loss: 0.0158\n",
      "Epoch 26/50\n",
      " - 118s - loss: 0.0153\n",
      "Epoch 27/50\n",
      " - 122s - loss: 0.0158\n",
      "Epoch 28/50\n",
      " - 133s - loss: 0.0155\n",
      "Epoch 29/50\n",
      " - 123s - loss: 0.0160\n",
      "Epoch 30/50\n",
      " - 120s - loss: 0.0154\n",
      "Epoch 31/50\n",
      " - 117s - loss: 0.0150\n",
      "Epoch 32/50\n",
      " - 117s - loss: 0.0151\n",
      "Epoch 33/50\n",
      " - 117s - loss: 0.0155\n",
      "Epoch 34/50\n",
      " - 116s - loss: 0.0153\n",
      "Epoch 35/50\n",
      " - 116s - loss: 0.0149\n",
      "Epoch 36/50\n",
      " - 116s - loss: 0.0150\n",
      "Epoch 37/50\n",
      " - 116s - loss: 0.0151\n",
      "Epoch 38/50\n",
      " - 116s - loss: 0.0152\n",
      "Epoch 39/50\n",
      " - 116s - loss: 0.0155\n",
      "Epoch 40/50\n",
      " - 116s - loss: 0.0150\n",
      "Epoch 41/50\n",
      " - 116s - loss: 0.0150\n",
      "Epoch 42/50\n",
      " - 116s - loss: 0.0154\n",
      "Epoch 43/50\n",
      " - 116s - loss: 0.0150\n",
      "Epoch 44/50\n",
      " - 116s - loss: 0.0147\n",
      "Epoch 45/50\n",
      " - 116s - loss: 0.0149\n",
      "Epoch 46/50\n",
      " - 116s - loss: 0.0151\n",
      "Epoch 47/50\n",
      " - 116s - loss: 0.0150\n",
      "Epoch 48/50\n",
      " - 116s - loss: 0.0149\n",
      "Epoch 49/50\n",
      " - 116s - loss: 0.0144\n",
      "Epoch 50/50\n",
      " - 116s - loss: 0.0150\n"
     ]
    }
   ],
   "source": [
    "#Aug with larger epochs\n",
    "\n",
    "hist_aug2 = model.fit_generator(BatchGenerator_withaug(fetcher),\n",
    "                    samples_per_epoch=steps_to_take*3, \n",
    "                    nb_epoch=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=[hist,checkpointer,early_stopping],\n",
    "                   )\n",
    "# 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 56s - loss: 0.0538\n",
      "Epoch 2/50\n",
      " - 54s - loss: 0.0327\n",
      "Epoch 3/50\n",
      " - 54s - loss: 0.0310\n",
      "Epoch 4/50\n",
      " - 54s - loss: 0.0298\n",
      "Epoch 5/50\n",
      " - 54s - loss: 0.0285\n",
      "Epoch 6/50\n",
      " - 54s - loss: 0.0270\n",
      "Epoch 7/50\n",
      " - 54s - loss: 0.0267\n",
      "Epoch 8/50\n",
      " - 54s - loss: 0.0257\n",
      "Epoch 9/50\n",
      " - 54s - loss: 0.0248\n",
      "Epoch 10/50\n",
      " - 54s - loss: 0.0248\n",
      "Epoch 11/50\n",
      " - 54s - loss: 0.0237\n",
      "Epoch 12/50\n",
      " - 54s - loss: 0.0231\n",
      "Epoch 13/50\n",
      " - 54s - loss: 0.0228\n",
      "Epoch 14/50\n",
      " - 54s - loss: 0.0224\n",
      "Epoch 15/50\n",
      " - 54s - loss: 0.0221\n",
      "Epoch 16/50\n",
      " - 54s - loss: 0.0220\n",
      "Epoch 17/50\n",
      " - 54s - loss: 0.0214\n",
      "Epoch 18/50\n",
      " - 54s - loss: 0.0221\n",
      "Epoch 19/50\n",
      " - 54s - loss: 0.0209\n",
      "Epoch 20/50\n",
      " - 54s - loss: 0.0210\n",
      "Epoch 21/50\n",
      " - 54s - loss: 0.0209\n",
      "Epoch 22/50\n",
      " - 54s - loss: 0.0207\n",
      "Epoch 23/50\n",
      " - 54s - loss: 0.0211\n",
      "Epoch 24/50\n",
      " - 54s - loss: 0.0207\n",
      "Epoch 25/50\n",
      " - 54s - loss: 0.0204\n",
      "Epoch 26/50\n",
      " - 54s - loss: 0.0198\n",
      "Epoch 27/50\n",
      " - 54s - loss: 0.0199\n",
      "Epoch 28/50\n",
      " - 55s - loss: 0.0198\n",
      "Epoch 29/50\n",
      " - 57s - loss: 0.0204\n",
      "Epoch 30/50\n",
      " - 55s - loss: 0.0191\n",
      "Epoch 31/50\n",
      " - 55s - loss: 0.0190\n",
      "Epoch 32/50\n",
      " - 55s - loss: 0.0191\n",
      "Epoch 33/50\n",
      " - 54s - loss: 0.0192\n",
      "Epoch 34/50\n",
      " - 54s - loss: 0.0189\n",
      "Epoch 35/50\n",
      " - 54s - loss: 0.0187\n",
      "Epoch 36/50\n",
      " - 54s - loss: 0.0183\n",
      "Epoch 37/50\n",
      " - 54s - loss: 0.0183\n",
      "Epoch 38/50\n",
      " - 54s - loss: 0.0185\n",
      "Epoch 39/50\n",
      " - 54s - loss: 0.0189\n",
      "Epoch 40/50\n",
      " - 54s - loss: 0.0186\n",
      "Epoch 41/50\n",
      " - 54s - loss: 0.0183\n",
      "Epoch 42/50\n",
      " - 54s - loss: 0.0185\n",
      "Epoch 43/50\n",
      " - 54s - loss: 0.0179\n",
      "Epoch 44/50\n",
      " - 54s - loss: 0.0178\n",
      "Epoch 45/50\n",
      " - 54s - loss: 0.0179\n",
      "Epoch 46/50\n",
      " - 54s - loss: 0.0177\n",
      "Epoch 47/50\n",
      " - 54s - loss: 0.0177\n",
      "Epoch 48/50\n",
      " - 54s - loss: 0.0179\n",
      "Epoch 49/50\n",
      " - 54s - loss: 0.0169\n",
      "Epoch 50/50\n",
      " - 54s - loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "hist_v = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take,\n",
    "                    nb_epoch=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "#                     validation_data=validGenerator(fetcher),\n",
    "#                     validation_steps=val_steps_to_take\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 54s - loss: 0.0161 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.01435\n",
      "Epoch 2/50\n",
      " - 54s - loss: 0.0159 - val_loss: 0.0140\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01435 to 0.01405, saving model to ./tmp/weights.hdf5\n",
      "Epoch 3/50\n",
      " - 54s - loss: 0.0154 - val_loss: 0.0155\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01405\n",
      "Epoch 4/50\n",
      " - 54s - loss: 0.0154 - val_loss: 0.0140\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01405 to 0.01400, saving model to ./tmp/weights.hdf5\n",
      "Epoch 5/50\n",
      " - 54s - loss: 0.0155 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01400\n",
      "Epoch 6/50\n",
      " - 54s - loss: 0.0158 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01400\n",
      "Epoch 7/50\n",
      " - 54s - loss: 0.0160 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01400\n",
      "Epoch 8/50\n",
      " - 54s - loss: 0.0157 - val_loss: 0.0149\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01400\n",
      "Epoch 9/50\n",
      " - 54s - loss: 0.0155 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01400\n",
      "Epoch 10/50\n",
      " - 54s - loss: 0.0157 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01400\n",
      "Epoch 11/50\n",
      " - 54s - loss: 0.0153 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01400\n",
      "Epoch 12/50\n",
      " - 54s - loss: 0.0151 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01400\n",
      "Epoch 13/50\n",
      " - 54s - loss: 0.0155 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01400\n",
      "Epoch 14/50\n",
      " - 54s - loss: 0.0153 - val_loss: 0.0155\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01400\n",
      "Epoch 15/50\n",
      " - 55s - loss: 0.0156 - val_loss: 0.0137\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01400 to 0.01371, saving model to ./tmp/weights.hdf5\n",
      "Epoch 16/50\n",
      " - 55s - loss: 0.0157 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01371\n",
      "Epoch 17/50\n",
      " - 56s - loss: 0.0149 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01371\n",
      "Epoch 18/50\n",
      " - 56s - loss: 0.0156 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01371\n",
      "Epoch 19/50\n",
      " - 58s - loss: 0.0154 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01371\n",
      "Epoch 20/50\n",
      " - 57s - loss: 0.0154 - val_loss: 0.0137\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01371 to 0.01370, saving model to ./tmp/weights.hdf5\n",
      "Epoch 21/50\n",
      " - 61s - loss: 0.0153 - val_loss: 0.0127\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01370 to 0.01267, saving model to ./tmp/weights.hdf5\n",
      "Epoch 22/50\n",
      " - 58s - loss: 0.0157 - val_loss: 0.0149\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01267\n",
      "Epoch 23/50\n",
      " - 57s - loss: 0.0159 - val_loss: 0.0118\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01267 to 0.01177, saving model to ./tmp/weights.hdf5\n",
      "Epoch 24/50\n",
      " - 64s - loss: 0.0154 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01177\n",
      "Epoch 25/50\n",
      " - 61s - loss: 0.0155 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01177\n",
      "Epoch 26/50\n",
      " - 56s - loss: 0.0150 - val_loss: 0.0149\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01177\n",
      "Epoch 27/50\n",
      " - 56s - loss: 0.0155 - val_loss: 0.0132\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01177\n",
      "Epoch 28/50\n",
      " - 56s - loss: 0.0152 - val_loss: 0.0136\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01177\n",
      "Epoch 29/50\n",
      " - 65s - loss: 0.0157 - val_loss: 0.0133\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01177\n",
      "Epoch 30/50\n",
      " - 69s - loss: 0.0150 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01177\n",
      "Epoch 31/50\n",
      " - 62s - loss: 0.0150 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01177\n",
      "Epoch 32/50\n",
      " - 57s - loss: 0.0149 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01177\n",
      "Epoch 33/50\n",
      " - 54s - loss: 0.0149 - val_loss: 0.0125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01177\n",
      "Epoch 34/50\n",
      " - 54s - loss: 0.0148 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01177\n",
      "Epoch 35/50\n",
      " - 54s - loss: 0.0144 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01177\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist_vaug = model.fit_generator(BatchGenerator_withaug(fetcher),\n",
    "                    samples_per_epoch=steps_to_take,\n",
    "                    nb_epoch=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=[hist_v2,checkpointer,early_stopping],\n",
    "                    validation_data=validGenerator(fetcher),\n",
    "                    validation_steps=val_steps_to_take\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "Model = load_model('tmp/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 327s - loss: 0.0134 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.01177\n",
      "Epoch 2/30\n",
      " - 327s - loss: 0.0130 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01177\n",
      "Epoch 3/30\n",
      " - 326s - loss: 0.0123 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01177\n",
      "Epoch 4/30\n",
      " - 327s - loss: 0.0124 - val_loss: 0.0181\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01177\n",
      "Epoch 5/30\n",
      " - 326s - loss: 0.0123 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01177\n",
      "Epoch 6/30\n",
      " - 326s - loss: 0.0122 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01177\n",
      "Epoch 7/30\n",
      " - 326s - loss: 0.0125 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01177\n",
      "Epoch 8/30\n",
      " - 327s - loss: 0.0120 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01177\n",
      "Epoch 9/30\n",
      " - 326s - loss: 0.0117 - val_loss: 0.0237\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01177\n",
      "Epoch 10/30\n",
      " - 327s - loss: 0.0119 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01177\n",
      "Epoch 11/30\n",
      " - 327s - loss: 0.0116 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01177\n",
      "Epoch 12/30\n",
      " - 326s - loss: 0.0113 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01177\n",
      "Epoch 13/30\n",
      " - 326s - loss: 0.0115 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01177\n",
      "Epoch 14/30\n",
      " - 326s - loss: 0.0115 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01177\n",
      "Epoch 15/30\n",
      " - 329s - loss: 0.0113 - val_loss: 0.0183\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01177\n",
      "Epoch 16/30\n",
      " - 326s - loss: 0.0112 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01177\n",
      "Epoch 17/30\n",
      " - 326s - loss: 0.0108 - val_loss: 0.0211\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01177\n",
      "Epoch 18/30\n",
      " - 326s - loss: 0.0114 - val_loss: 0.0276\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01177\n",
      "Epoch 19/30\n",
      " - 326s - loss: 0.0109 - val_loss: 0.0155\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01177\n",
      "Epoch 20/30\n"
     ]
    }
   ],
   "source": [
    "hist_vaug = model.fit_generator(BatchGenerator_withaug(fetcher),\n",
    "                    samples_per_epoch=steps_to_take*6,\n",
    "                    nb_epoch=30,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                    validation_data=validGenerator(fetcher),\n",
    "                    validation_steps=val_steps_to_take\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.test_images_paths:\n",
    "            X_test = process_images([getter.test_path + '/' + fname for fname in [f]])\n",
    "            X_test = np.reshape(X_test, (1,106,106,3))\n",
    "            yield (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(testGenerator(fetcher),\n",
    "                       val_samples = len(fetcher.test_images_paths),\n",
    "                        max_q_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 37)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = open('all_zeros_benchmark.csv','r').readlines()[0]\n",
    "\n",
    "with open('submission_3.csv','w') as outfile:\n",
    "    outfile.write(header)\n",
    "    for i in range(len(fetcher.test_images_paths)):\n",
    "        id_ = (fetcher.get_id(fetcher.test_images_paths[i]))\n",
    "        pred = predictions[i]\n",
    "        outline = id_ + \",\" + \",\".join([str(x) for x in pred])\n",
    "        outfile.write(outline + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
