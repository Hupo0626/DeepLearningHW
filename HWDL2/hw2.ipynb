{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-GY 9223-D: Deep Learning Homework 2\n",
    "Due on Friday, 12th March 2019, 11:55 PM\n",
    "\n",
    "This homework can be done in pairs.\n",
    "\n",
    "Write down the UNIs (NetIDs) of your group (if applicable)\n",
    "\n",
    "Member 1: Hupo Tang, ht1073\n",
    "\n",
    "Member 2: Name, NetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, Lambda, Reshape, ZeroPadding2D\n",
    "# from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import cv2\n",
    "# import matplotlib.pylab as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "TRAIN = './images_training_revtest/'\n",
    "TEST = './images_test_rev1/'\n",
    "LABELS = './training_solutions_rev1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = glob.glob(data_path+'images_training_revtest/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(10):\n",
    "    os.rename(shuf[i], data_path+ 'valid/' + shuf[i].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from scipy.misc import imresize\n",
    "import csv\n",
    "\n",
    "class data_loader:    \n",
    "    \"\"\"\n",
    "    Creates a class for handling train/valid/test data paths,\n",
    "    training labels and image IDs.\n",
    "    Useful for switching between sample and full datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):    \n",
    "        self.path = path \n",
    "        self.train_path = TRAIN\n",
    "        #self.val_path = path + \"valid\"\n",
    "        self.test_path = TEST\n",
    "        \n",
    "        def get_paths(directory):\n",
    "            return [f for f in os.listdir(directory)]\n",
    "        \n",
    "        self.training_images_paths = get_paths(self.train_path)\n",
    "        # self.validation_images_paths = get_paths(self.val_path)\n",
    "        self.test_images_paths = get_paths(self.test_path)    \n",
    "        \n",
    "        def get_all_solutions():\n",
    "        # Import solutions file and load into self.solutions\n",
    "            all_solutions = {}\n",
    "            # /'training_solutions_rev1.csv'\n",
    "            with open(LABELS, 'r') as f:\n",
    "                reader = csv.reader(f, delimiter=\",\")\n",
    "                next(reader)\n",
    "                for i, line in enumerate(reader):\n",
    "                    all_solutions[line[0]] = [float(x) for x in line[1:]]\n",
    "            return all_solutions\n",
    "        \n",
    "        self.all_solutions = get_all_solutions()\n",
    "\n",
    "    def get_id(self,fname):\n",
    "        return fname.replace(\".jpg\",\"\").replace(\"data\",\"\")\n",
    "        \n",
    "    def find_label(self,val):\n",
    "        return self.all_solutions[val]\n",
    "\n",
    "def rotate90(img):\n",
    "    return np.rot90(img,axes=(1,2))\n",
    "\n",
    "def rotate180(img):\n",
    "    tmp = np.rot90(img,axes=(1,2))\n",
    "    return np.rot90(tmp,axes=(1,2))\n",
    "\n",
    "def rotate270(img):\n",
    "    tmp = np.rot90(img,axes=(1,2))\n",
    "    tmp = np.rot90(tmp,axes=(1,2))\n",
    "    return np.rot90(tmp,axes=(1,2))\n",
    "\n",
    "def flip_h(img):\n",
    "    return np.flip(img, axis=1)\n",
    "\n",
    "def flip_v(img):\n",
    "    return np.flip(img, axis=2)\n",
    "\n",
    "def process_images(paths,aug=0):\n",
    "    \"\"\"\n",
    "    Import image at 'paths', decode, centre crop and prepare for batching. \n",
    "    \"\"\"\n",
    "    count = len(paths)\n",
    "#     print(count)\n",
    "    arr = np.zeros(shape=(count,3,69,69))\n",
    "    for c, path in enumerate(paths):\n",
    "        raw_img = cv2.imread(path).T  # 3,424,424\n",
    "#         print(raw_img.shape)\n",
    "#         img = raw_img\n",
    "        if not aug:\n",
    "            img=raw_img\n",
    "        else:\n",
    "            auger={1:rotate90,2:rotate180,3:rotate270,4:flip_h,5:flip_v}.get(aug)\n",
    "            img=auger(raw_img)\n",
    "        img = img[:,108:315,108:315] #crop 424x424 -> 207x207\n",
    "        img = imresize(img,size=(69,69,3),interp=\"cubic\").T # downsample to half res\n",
    "        \n",
    "#         print(img.shape)\n",
    "        img1 = np.sqrt(img[:1,:,:]*255)\n",
    "        img2 = np.sqrt(img[1:2,:,:])\n",
    "        img2 = np.append(img1, img2, axis=0)\n",
    "        img3 = np.sqrt(img[2:3,:,:])\n",
    "        img = np.append(img2, img3.reshape((1,69,69)), axis=0)\n",
    "#         print(img.shape)\n",
    "        \n",
    "        arr[c] = img\n",
    "#     print(arr.shape)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def BatchGenerator(getter):\n",
    "    while 1:\n",
    "        for f in getter.training_images_paths:\n",
    "            for aug in {0,1,2,3,4,5}:\n",
    "                X_train = process_images([getter.train_path + '/' + fname for fname in [f]], aug)\n",
    "                X_train = np.reshape(X_train,(1,69,69,3))\n",
    "                id_ = getter.get_id(f)\n",
    "                y_train = np.array(getter.find_label(id_))\n",
    "                y_train = np.reshape(y_train,(1,37))\n",
    "                assert(X_train.shape==(1,69,69,3))\n",
    "                yield (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = data_loader(data_path)\n",
    "# fetcher.training_images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG_16 Network\n",
    "\n",
    "def ConvBlock(layers, model, filters):\n",
    "    \"\"\"\n",
    "    Create a layered Conv/Pooling block\n",
    "    \"\"\"\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))  # zero padding of size 1\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))  # 3x3 filter size \n",
    "    model.add(MaxPooling2D((1,1), strides=(2,2)))\n",
    "\n",
    "def FCBlock(model):\n",
    "    \"\"\"\n",
    "    Fully connected block with ReLU and dropout\n",
    "    \"\"\"\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "def VGG_16():\n",
    "    \"\"\"\n",
    "    Implement VGG16 architecture\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x : x, input_shape=(3,69,69)))\n",
    "    \n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    \n",
    "    model.add(Dense(37, activation = 'sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2, 35, 64)\n",
      "(None, 1, 5, 512)\n"
     ]
    }
   ],
   "source": [
    "# Compile \n",
    "optimizer = RMSprop(lr=1e-6)\n",
    "model = VGG_16()\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyCNN():\n",
    "    model = Sequential()\n",
    "    #NHWC(,69,69,3)\n",
    "    model.add(Lambda(lambda x: x, input_shape=(69,69,3)))\n",
    "#     print(model.output_shape)    \n",
    "#     model.add(ZeroPadding2D((2,2)))\n",
    "    model.add(Convolution2D(32, 6, 6, activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "#     model.add(ZeroPadding2D((2,2)))\n",
    "    model.add(Convolution2D(64, 5, 5, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(37, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile \n",
    "# optimizer = RMSprop(lr=1e-5)\n",
    "myModel = MyCNN()\n",
    "myModel.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath='./weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size = 32\n",
    "steps_to_take = int(len(fetcher.training_images_paths)*6/batch_size)\n",
    "#val_steps_to_take = int(len(fetcher.validation_images_paths)/batch_size)\n",
    "                #typically be equal to the number of unique samples if your dataset\n",
    "                #divided by the batch size.\n",
    "\n",
    "#model = load_model('tmp/weights.hdf5')\n",
    "\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 20s - loss: 0.1813\n",
      "Epoch 2/5\n",
      " - 21s - loss: 0.1919\n",
      "Epoch 3/5\n",
      " - 22s - loss: 0.1780\n",
      "Epoch 4/5\n",
      " - 24s - loss: 0.1777\n",
      "Epoch 5/5\n",
      " - 23s - loss: 0.1634\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=5,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 26s - loss: 0.0651\n",
      "Epoch 2/20\n",
      " - 19s - loss: 0.0513\n",
      "Epoch 3/20\n",
      " - 16s - loss: 0.0633\n",
      "Epoch 4/20\n",
      " - 16s - loss: 0.0653\n",
      "Epoch 5/20\n",
      " - 16s - loss: 0.0507\n",
      "Epoch 6/20\n",
      " - 16s - loss: 0.0634\n",
      "Epoch 7/20\n",
      " - 16s - loss: 0.0640\n",
      "Epoch 8/20\n",
      " - 16s - loss: 0.0486\n",
      "Epoch 9/20\n",
      " - 16s - loss: 0.0626\n",
      "Epoch 10/20\n",
      " - 17s - loss: 0.0503\n",
      "Epoch 11/20\n",
      " - 16s - loss: 0.0528\n",
      "Epoch 12/20\n",
      " - 17s - loss: 0.0472\n",
      "Epoch 13/20\n",
      " - 16s - loss: 0.0489\n",
      "Epoch 14/20\n",
      " - 15s - loss: 0.0709\n",
      "Epoch 15/20\n",
      " - 16s - loss: 0.0518\n",
      "Epoch 16/20\n",
      " - 16s - loss: 0.0646\n",
      "Epoch 17/20\n",
      " - 16s - loss: 0.0464\n",
      "Epoch 18/20\n",
      " - 16s - loss: 0.0595\n",
      "Epoch 19/20\n",
      " - 15s - loss: 0.0551\n",
      "Epoch 20/20\n",
      " - 16s - loss: 0.0554\n"
     ]
    }
   ],
   "source": [
    "hist2 = myModel.fit_generator(BatchGenerator(fetcher),\n",
    "                    samples_per_epoch=steps_to_take, \n",
    "                    nb_epoch=20,\n",
    "                    verbose=2,\n",
    "                    callbacks=[history,checkpointer,early_stopping],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "Epoch 1/5\n",
      " - 27s - loss: 0.0943\n",
      "Epoch 2/5\n",
      " - 22s - loss: 0.0899\n",
      "Epoch 3/5\n",
      " - 21s - loss: 0.0900\n",
      "Epoch 4/5\n",
      " - 23s - loss: 0.0871\n",
      "Epoch 5/5\n",
      " - 22s - loss: 0.0832\n"
     ]
    }
   ],
   "source": [
    "print(steps_to_take)\n",
    "hist2 = myModel.fit_generator(BatchGenerator(fetcher),\n",
    "          samples_per_epoch=steps_to_take,\n",
    "          epochs=5,\n",
    "          verbose=2,\n",
    "          callbacks=[history,checkpointer,early_stopping],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11*10//9+8-7*6//5+4-3*2//1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
